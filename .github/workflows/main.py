import os
import time
import html
import textwrap
from urllib.parse import quote_plus
import requests
import feedparser

# ------------ Env & defaults ------------
BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN", "").strip()
CHAT_ID = os.getenv("TELEGRAM_CHAT_ID", "").strip()

ACTIVE_PLATFORMS = (os.getenv("ACTIVE_PLATFORMS", "reddit,hn,youtube")
                    .lower().replace(" ", ""))
# keywords "ayaqqabÄ±/paltar/..." kimi bÃ¶lÃ¼nÃ¼r
KEYWORDS_RAW = os.getenv("KEYWORDS", "ayaqqabÄ±/telefon").strip()
KEYWORDS = [k for k in KEYWORDS_RAW.split("/") if k]
DAILY_LIMIT = int(os.getenv("DAILY_LIMIT", "50"))

if not BOT_TOKEN or not CHAT_ID:
    print(f"ENV check: TOK={'OK' if BOT_TOKEN else 'MISSING'} "
          f"CID={'OK' if CHAT_ID else 'MISSING'} "
          f"PLATFORMS={ACTIVE_PLATFORMS} KW={KEYWORDS_RAW} LIMIT={DAILY_LIMIT}")
    raise SystemExit(1)

TG_API = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"


# ------------ Helpers ------------
def send_telegram(text: str):
    """Split-safe Telegram sender (HTML)."""
    text = text.strip()
    if not text:
        return
    # Telegram message limit ~4096
    chunks = [text[i:i+3500] for i in range(0, len(text), 3500)]
    for chunk in chunks:
        resp = requests.post(
            TG_API,
            data={
                "chat_id": CHAT_ID,
                "text": chunk,
                "parse_mode": "HTML",
                "disable_web_page_preview": True,
            },
            timeout=30,
        )
        if resp.status_code != 200:
            print("Telegram error:", resp.status_code, resp.text)
        time.sleep(0.6)


def fmt_item(title: str, link: str, src: str) -> str:
    title = html.escape(title or "").strip()
    link = link.strip()
    return f"â€¢ <b>{src}</b>: <a href=\"{link}\">{title}</a>"


# ------------ Fetchers (RSS) ------------
def fetch_reddit(keyword: str, max_items: int):
    url = f"https://www.reddit.com/search.rss?q={quote_plus(keyword)}&sort=new"
    d = feedparser.parse(url)
    out = []
    for e in d.entries[:max_items]:
        title = getattr(e, "title", "")
        link = getattr(e, "link", "")
        if title and link:
            out.append(("reddit", title, link))
    return out


def fetch_hn(keyword: str, max_items: int):
    # hnrss.org â€“ Algolia search
    url = f"https://hnrss.org/newest?q={quote_plus(keyword)}"
    d = feedparser.parse(url)
    out = []
    for e in d.entries[:max_items]:
        title = getattr(e, "title", "")
        link = getattr(e, "link", "")
        if title and link:
            out.append(("hn", title, link))
    return out


def fetch_youtube(keyword: str, max_items: int):
    # YouTube search RSS (rÉ™smi olmasa da iÅŸlÉ™yir)
    url = f"https://www.youtube.com/feeds/videos.xml?search_query={quote_plus(keyword)}"
    d = feedparser.parse(url)
    out = []
    for e in d.entries[:max_items]:
        title = getattr(e, "title", "")
        link = getattr(e, "link", "")
        if title and link:
            out.append(("youtube", title, link))
    return out


# Placeholder-lar (hazÄ±rda RSS yoxdur)
def fetch_placeholder(name: str, *_args, **_kw):
    # Burada heÃ§ nÉ™ qaytarmÄ±rÄ±q â€“ sadÉ™cÉ™ atlayÄ±rÄ±q
    print(f"{name}: rÉ™smi RSS yoxdur, atlanÄ±r.")
    return []


FETCHERS = {
    "reddit": fetch_reddit,
    "hn": fetch_hn,            # Hacker News
    "hackernews": fetch_hn,
    "youtube": fetch_youtube,
    "producthunt": lambda kw, n: fetch_placeholder("producthunt"),
    "instagram": lambda kw, n: fetch_placeholder("instagram"),
    "tiktok": lambda kw, n: fetch_placeholder("tiktok"),
    "threads": lambda kw, n: fetch_placeholder("threads"),
}

# HansÄ±lar aktivdirsÉ™, sÄ±rala
PLATFORMS = [p for p in ACTIVE_PLATFORMS.split(",") if p in FETCHERS]


# ------------ Main ------------
def main():
    # SÃ¼rÉ™tli â€œpingâ€
    send_telegram("âœ… <b>GitHub Actions bot</b> baÅŸladÄ±. AxtarÄ±ÅŸa keÃ§irÉ™mâ€¦")

    total_sent = 0
    report_lines = []
    seen_links = set()

    for platform in PLATFORMS:
        fn = FETCHERS[platform]
        platform_count = 0

        for kw in KEYWORDS:
            if total_sent >= DAILY_LIMIT:
                break
            max_per_kw = max(1, min(10, DAILY_LIMIT - total_sent))
            try:
                items = fn(kw, max_per_kw)
            except Exception as e:
                print(f"{platform}/{kw} fetch error: {e}")
                continue

            for (_src, title, link) in items:
                if total_sent >= DAILY_LIMIT:
                    break
                if link in seen_links:
                    continue
                seen_links.add(link)
                report_lines.append(fmt_item(title, link, platform))
                total_sent += 1
                platform_count += 1

        print(f"{platform}: {platform_count} nÉ™ticÉ™ toplandÄ±.")

    if report_lines:
        header = f"ğŸ” <b>AxtarÄ±ÅŸ tamamlandÄ±</b>\n" \
                 f"Platformalar: <code>{', '.join(PLATFORMS) or 'â€”'}</code>\n" \
                 f"AÃ§ar sÃ¶zlÉ™r: <code>{', '.join(KEYWORDS) or 'â€”'}</code>\n" \
                 f"Limit: <code>{DAILY_LIMIT}</code>\n\n"
        # Ã‡ox uzundursa bÃ¶lÃ¼b gÃ¶ndÉ™r
        payload = header + "\n".join(report_lines)
        for chunk in textwrap.wrap(payload, 3500, break_long_words=False, break_on_hyphens=False):
            send_telegram(chunk)
    else:
        send_telegram("â„¹ï¸ HeÃ§ nÉ™ tapÄ±lmadÄ± (RSS mÉ™nbÉ™lÉ™rindÉ™ uyÄŸun nÉ™ticÉ™ yoxdur).")


if __name__ == "__main__":
    main()
